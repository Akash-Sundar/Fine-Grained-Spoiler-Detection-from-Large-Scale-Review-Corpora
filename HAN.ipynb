{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aKMyWWdZB1o",
        "outputId": "6987fb22-e275-4809-c058-15870ccd8101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoxNfxQ4kj34"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import keras\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ['KERAS_BACKEND']='tensorflow'\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras import initializers\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorflow_version 1.15\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XDyCoe0QaZjE",
        "outputId": "34a9c2e7-ad10-46ea-fc82-687288e9d6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZA_AbPPuklZq",
        "outputId": "def8758d-a557-46c9-fd81-3ee883a5f641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q45rZO3IoOpP",
        "outputId": "6ae75341-444e-4609-9b1b-0aaacb121036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spoiler Net Implementation"
      ],
      "metadata": {
        "id": "M0gVEDooIL8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/X_train.pkl')\n",
        "X_val = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/X_val.pkl')\n",
        "X_test = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/X_test.pkl')\n",
        "\n",
        "y_train = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/y_train.pkl')\n",
        "y_val = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/y_val.pkl')\n",
        "y_test = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/y_test.pkl')"
      ],
      "metadata": {
        "id": "WrTBdGVvSqyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum([y_train == 0]))\n",
        "print(np.sum([y_train == 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktgx9AoD6ioO",
        "outputId": "78041eab-ebfe-4f8a-da85-721c88b6bfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "290487\n",
            "9513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yetjuXmD6o5L",
        "outputId": "303dbe9b-2a9c-401e-9e0b-bd9149dc9906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.concat([pd.concat([X_train,X_val], axis = 0), pd.concat([y_train, y_val], axis = 0)], axis = 1)\n",
        "data_train.head(5)"
      ],
      "metadata": {
        "id": "4nq9C5dek1Nc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "1475f47f-b033-4c20-d534-92196a6d400e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0   timestamp  review_id  user_id  book_id  review_rating  \\\n",
              "34894        34894  2013-05-08     618798    13045     8859              4   \n",
              "65566        65566  2014-04-04     104490     6051    16790              5   \n",
              "126414      126414  2013-09-17     608006    12078    12394              5   \n",
              "186864      186864  2016-05-23    1207235     1210    17236              4   \n",
              "1114          1114  2016-09-23     653787    12763     3401              0   \n",
              "\n",
              "                                                 sentence  _pos_sentence  \\\n",
              "34894   She has no understanding of why, only that she...             10   \n",
              "65566        Will defiantly be reading more of aly's work              3   \n",
              "126414  This is not an easy girl meets boy, they giggl...             16   \n",
              "186864                                             -Cesar             28   \n",
              "1114                                 love alien romances.              1   \n",
              "\n",
              "        _num_sentences                                 sentence_embedding  \\\n",
              "34894               21  [0.12825826, -0.015271957, 0.014717576, 0.0586...   \n",
              "65566                4  [-0.04218291, -0.043608442, 0.017529905, 0.009...   \n",
              "126414              21  [-0.07032201, 0.008057908, 0.055432968, 0.0241...   \n",
              "186864              29  [-0.06680018, 0.031215038, 0.008926821, 0.0324...   \n",
              "1114                 3  [-0.030931823, -0.03957359, 0.047186073, 0.046...   \n",
              "\n",
              "        is_spoiler  \n",
              "34894            0  \n",
              "65566            0  \n",
              "126414           0  \n",
              "186864           0  \n",
              "1114             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1df4a2ac-38d6-4213-9763-097e088076c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>review_rating</th>\n",
              "      <th>sentence</th>\n",
              "      <th>_pos_sentence</th>\n",
              "      <th>_num_sentences</th>\n",
              "      <th>sentence_embedding</th>\n",
              "      <th>is_spoiler</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34894</th>\n",
              "      <td>34894</td>\n",
              "      <td>2013-05-08</td>\n",
              "      <td>618798</td>\n",
              "      <td>13045</td>\n",
              "      <td>8859</td>\n",
              "      <td>4</td>\n",
              "      <td>She has no understanding of why, only that she...</td>\n",
              "      <td>10</td>\n",
              "      <td>21</td>\n",
              "      <td>[0.12825826, -0.015271957, 0.014717576, 0.0586...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65566</th>\n",
              "      <td>65566</td>\n",
              "      <td>2014-04-04</td>\n",
              "      <td>104490</td>\n",
              "      <td>6051</td>\n",
              "      <td>16790</td>\n",
              "      <td>5</td>\n",
              "      <td>Will defiantly be reading more of aly's work</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>[-0.04218291, -0.043608442, 0.017529905, 0.009...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126414</th>\n",
              "      <td>126414</td>\n",
              "      <td>2013-09-17</td>\n",
              "      <td>608006</td>\n",
              "      <td>12078</td>\n",
              "      <td>12394</td>\n",
              "      <td>5</td>\n",
              "      <td>This is not an easy girl meets boy, they giggl...</td>\n",
              "      <td>16</td>\n",
              "      <td>21</td>\n",
              "      <td>[-0.07032201, 0.008057908, 0.055432968, 0.0241...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186864</th>\n",
              "      <td>186864</td>\n",
              "      <td>2016-05-23</td>\n",
              "      <td>1207235</td>\n",
              "      <td>1210</td>\n",
              "      <td>17236</td>\n",
              "      <td>4</td>\n",
              "      <td>-Cesar</td>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "      <td>[-0.06680018, 0.031215038, 0.008926821, 0.0324...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>1114</td>\n",
              "      <td>2016-09-23</td>\n",
              "      <td>653787</td>\n",
              "      <td>12763</td>\n",
              "      <td>3401</td>\n",
              "      <td>0</td>\n",
              "      <td>love alien romances.</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>[-0.030931823, -0.03957359, 0.047186073, 0.046...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1df4a2ac-38d6-4213-9763-097e088076c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1df4a2ac-38d6-4213-9763-097e088076c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1df4a2ac-38d6-4213-9763-097e088076c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "id": "E4wlNdKn91Ry",
        "outputId": "1695f030-76a3-42f6-9f41-02ae2805105d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350000, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENT_LENGTH = 100\n",
        "MAX_SENTS = 25\n",
        "MAX_NB_WORDS = 20000  ## dictionary size\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.15\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    string = string.strip().lower()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    trans_string = ' '.join([word for word in string.strip().split() if word not in stopWords])\n",
        "    return trans_string"
      ],
      "metadata": {
        "id": "aGk2Z4s3k29K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XDEIBm75WBO",
        "outputId": "e1c3956e-9358-4197-dc9c-2d51c65bc1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'timestamp', 'review_id', 'user_id', 'book_id',\n",
              "       'review_rating', 'sentence', '_pos_sentence', '_num_sentences',\n",
              "       'sentence_embedding', 'is_spoiler'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTbwvXhM7Z0i",
        "outputId": "d417be6a-a7ac-4449-930e-bcabc9f565b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvaM31BB_HR9",
        "outputId": "7224f71e-3d8f-4c0e-ebcf-01dff048b421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34894     She has no understanding of why, only that she...\n",
              "65566          Will defiantly be reading more of aly's work\n",
              "126414    This is not an easy girl meets boy, they giggl...\n",
              "186864                                               -Cesar\n",
              "1114                                   love alien romances.\n",
              "                                ...                        \n",
              "381658    Each new plot line or setting is beautifully c...\n",
              "312288                                            3.5 Stars\n",
              "51649     There's an incredible amount of time spent on ...\n",
              "221593    This was a great addition, and I can't wait to...\n",
              "160422    This book is much more raw and intense than Jo...\n",
              "Name: sentence, Length: 350000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.sentence.iloc[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2c2heT3z_QId",
        "outputId": "e1ac170c-9dd9-49dd-ef08-3a4377d14c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is not an easy girl meets boy, they giggle and flirt, and live happily ever story.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "reviews = []\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for idx in range(0,len(data_train.sentence)):\n",
        "    text = data_train.sentence.iloc[idx]\n",
        "    text = BeautifulSoup(text, \"lxml\")\n",
        "    text = clean_str(text.get_text())\n",
        "    texts.append(text)\n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    reviews.append(sentences)\n",
        "\n",
        "    labels.append(data_train.review_rating.iloc[idx])\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)           ## top nb words in the reviews"
      ],
      "metadata": {
        "id": "ALh8IKSek5Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "7lUa54TRk7t_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e717a4b-65ca-401c-e6bc-d99d1096902f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69266"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sentences in enumerate(reviews):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j< MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                    data[i,j,k] = tokenizer.word_index[word]\n",
        "                    k=k+1\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Total %s unique tokens.' % len(word_index))\n",
        "\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "labels = to_categorical(np.asarray(labels), num_classes = 5)\n",
        "labels = labels.reshape([-1,5])\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "metadata": {
        "id": "G0xc4wNEk9RB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd882ac-792d-4a05-e947-faf8acfd6f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 69266 unique tokens.\n",
            "Shape of data tensor: (350000, 25, 100)\n",
            "Shape of label tensor: (2100000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]"
      ],
      "metadata": {
        "id": "Eh34J2Ojk_Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download glove embeddings from https://nlp.stanford.edu/projects/glove/\n",
        "GLOVE_DIR = \"/content/drive/MyDrive/CIS520 Final Project/\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "PYz3C8cplBS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7444bcf6-ef78-49a9-8628-4dd20db092b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# building Hierachical Attention network\n",
        "embedding_matrix = np.random.random((MAX_NB_WORDS + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i < MAX_NB_WORDS:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "whcbGJ20lDGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        #self.input_spec = [InputSpec(ndim=3)]\n",
        "        self.supports_masking = True\n",
        "        self.mask_zero = True\n",
        "        super(AttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape)==3\n",
        "        self.W = self.init((input_shape[-1],))\n",
        "        #self.trainable_weights = [self.W]\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = K.tanh(dot_product(x, self.W))\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None"
      ],
      "metadata": {
        "id": "hj-T9AHtlExf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "NWGRXnuzEm2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.compat.v1.placeholder(\n",
        "#    tf.float64\n",
        "#)"
      ],
      "metadata": {
        "id": "v_MscNzcCmny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(MAX_NB_WORDS + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=False)\n",
        "\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "l_lstm = Bidirectional(GRU(50, return_sequences=True))(embedded_sequences)\n",
        "l_dense = TimeDistributed(Dense(100))(l_lstm)\n",
        "l_att = AttLayer()(l_dense)\n",
        "sentEncoder = Model(sentence_input, l_att)\n",
        "sentEncoder.summary()\n",
        "\n",
        "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype = 'int32')\n",
        "#review_encoder = TimeDistributed(sentEncoder.get_layer(index = 0))(review_input)\n",
        "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
        "#review_encoder = tf.reshape(review_encoder, [-1,25,100])\n",
        "l_lstm_sent = Bidirectional(GRU(50, return_sequences=True))(review_encoder)\n",
        "l_dense_sent = TimeDistributed(Dense(100))(l_lstm_sent)\n",
        "l_att_sent = AttLayer()(l_dense_sent)\n",
        "preds = Dense(5, activation='softmax')(l_att_sent)\n",
        "model = Model(review_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9GEeCX_rlHrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e020058-e7b2-411b-8bab-8833bf35b8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 100)          2000100   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 100, 100)         45600     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 100, 100)         10100     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " att_layer (AttLayer)        (None, 100)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,055,800\n",
            "Trainable params: 55,700\n",
            "Non-trainable params: 2,000,100\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 25, 100)]         0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 25, 100)          2055800   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 25, 100)          45600     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 25, 100)          10100     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " att_layer_1 (AttLayer)      (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,112,005\n",
            "Trainable params: 111,905\n",
            "Non-trainable params: 2,000,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "id": "vqhRlMqafYTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72222d70-62ca-454f-9611-3284d0d08d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name:  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_f1(y_true, y_pred):\n",
        "    def recall_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "        recall = TP / (Positives+K.epsilon())\n",
        "        return recall\n",
        "\n",
        "\n",
        "    def precision_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = TP / (Pred_Positives+K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
        "\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "jvS3N0L1wLBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "PDuPQW8Axe0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics import AUC, Recall, Precision\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy', Precision(), Recall(), AUC(), custom_f1])\n",
        "\n",
        "callback = [EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=1,\n",
        "                              verbose=0, mode='auto')]\n",
        "\n",
        "print(\"model fitting - Hierachical attention network\")\n",
        "print(sentEncoder.summary())\n",
        "print(model.summary())\n",
        "\n",
        "#model.fit(x_train, y_train, validation_data=(x_val, y_val), \n",
        "#          epochs=1, batch_size=100, callbacks = callback)\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=1, batch_size=100, callbacks = callback)\n",
        "\n",
        "model.save('/content/drive/MyDrive/CIS520 Final Project/')"
      ],
      "metadata": {
        "id": "_A_OFdAAlJpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a94d006-010f-4bce-9aac-fe12d78fa808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model fitting - Hierachical attention network\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 100, 100)          2000100   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 100, 100)         45600     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 100, 100)         10100     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " att_layer_2 (AttLayer)      (None, 100)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,055,800\n",
            "Trainable params: 55,700\n",
            "Non-trainable params: 2,000,100\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 25, 100)]         0         \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, 25, 100)          2055800   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 25, 100)          45600     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDis  (None, 25, 100)          10100     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " att_layer_3 (AttLayer)      (None, 100)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,112,005\n",
            "Trainable params: 111,905\n",
            "Non-trainable params: 2,000,100\n",
            "_________________________________________________________________\n",
            "None\n",
            "2975/2975 [==============================] - 10625s 4s/step - loss: 0.1816 - accuracy: 0.8326 - precision: 0.8329 - recall: 0.8326 - auc: 0.9583 - custom_f1: 0.8326 - val_loss: 0.1785 - val_accuracy: 0.8361 - val_precision: 0.8361 - val_recall: 0.8361 - val_auc: 0.9590 - val_custom_f1: 0.8361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_10_layer_call_fn, gru_cell_10_layer_call_and_return_conditional_losses, gru_cell_11_layer_call_fn, gru_cell_11_layer_call_and_return_conditional_losses, gru_cell_7_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "uUlEva_pxhwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/CIS520 Final Project/', custom_objects={'custom_f1':custom_f1})"
      ],
      "metadata": {
        "id": "UuIkXpmDuVWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/X_test.pkl')\n",
        "y_test = pd.read_pickle('/content/drive/MyDrive/CIS520 Final Project/y_test.pkl')\n",
        "data_test = pd.concat([X_test, y_test], axis = 1)\n",
        "data_train = data_test\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "reviews = []\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for idx in range(0,len(X_test.sentence)):\n",
        "    text = X_test.sentence.iloc[idx]\n",
        "    text = BeautifulSoup(text, \"lxml\")\n",
        "    text = clean_str(text.get_text())\n",
        "    texts.append(text)\n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    reviews.append(sentences)\n",
        "\n",
        "    labels.append(X_test.review_rating.iloc[idx])\n",
        "\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)           ## top nb words in the reviews\n",
        "\n",
        "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "for i, sentences in enumerate(reviews):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j< MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                    data[i,j,k] = tokenizer.word_index[word]\n",
        "                    k=k+1\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Total %s unique tokens.' % len(word_index))\n",
        "\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "labels = to_categorical(np.asarray(labels), num_classes = 5)\n",
        "labels = labels.reshape([-1,5])\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# building Hierachical Attention network\n",
        "embedding_matrix = np.random.random((MAX_NB_WORDS + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i < MAX_NB_WORDS:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(MAX_NB_WORDS + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=False)\n",
        "\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "X_test = data\n",
        "y_test = labels\n",
        "\n",
        "#X_test = embedded_sequences"
      ],
      "metadata": {
        "id": "vvfUIWkYx6uT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bfccbf-230a-4196-951e-1816a29632e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 29011 unique tokens.\n",
            "Shape of data tensor: (50000, 25, 100)\n",
            "Shape of label tensor: (300000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from keras.metrics import AUC, Recall, Precision, Accuracy\n",
        "\n",
        "y_pred = y_pred.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "print('AUC: ', AUC(y_pred, y_test))\n",
        "print('Accuracy: ', Accuracy(y_pred, y_test))\n",
        "print('Precision: ', Precision(y_pred, y_test))\n",
        "print('Recall: ', Recall(y_pred, y_test))\n",
        "print('F1: ', custom_f1(y_pred, y_test))"
      ],
      "metadata": {
        "id": "SOv3_E7wxbJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "9cac4bd6-17df-4e0f-f75c-218ca730fdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 688s 440ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-7e62c5bfbeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AUC: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   return tf.__internal__.decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/metrics/metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_thresholds, curve, summation_method, name, dtype, thresholds, multi_label, num_labels, label_weights, from_logits)\u001b[0m\n\u001b[1;32m   1616\u001b[0m               np.array([0.0] + thresholds + [1.0])))\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mnum_thresholds\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m         raise ValueError('Argument `num_thresholds` must be an integer > 1. '\n\u001b[1;32m   1620\u001b[0m                          f'Received: num_thresholds={num_thresholds}')\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score as AUC, accuracy_score as Accuracy, precision_score as Precision,recall_score as Recall\n",
        "\n",
        "print('AUC: ', AUC(y_test, y_pred))\n",
        "#print('Accuracy: ', Accuracy(y_test, y_pred, ))\n",
        "#print('Precision: ', Precision(y_test, y_pred))\n",
        "#print('Recall: ', Recall(y_test, y_pred))\n",
        "print('F1: ', custom_f1(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_32vy4jk3uZG",
        "outputId": "4a9292d0-14da-49ae-807b-8a81137253e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC:  0.9584553622\n",
            "F1:  tf.Tensor(0.83334, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klJkG5tE3oli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiu0kD0OziCp",
        "outputId": "583b5470-e7e7-4ca6-ef27-ea16cb69211b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250000,)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9IfALR-ywXd",
        "outputId": "fce81b1c-63a9-4e1b-8ce2-309191239df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250000,)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.zeros((1, MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "## sample reviews\n",
        "\n",
        "## rating = 1\n",
        "#rev = 'Described as \"super soft\" and \"Exceptionally heavy weight\". But what I received looked more like a set of cheap towels coming from a discount store, and looked neither \"super soft\" nor \"Exceptionally heavy weight\". My order came from Warehouse Deals and was packed in a transparent plastic bag, so maybe a mistake was made?Anyway they are being sent back..'\n",
        "\n",
        "## rating = 4\n",
        "#rev = \"I guess after looking at other reviews that we got lucky  to have great service. Nicole was our server and she first of all informed that the computers were down so orders were going in slowly. Secondly, my wife ordered a lamb burger to be cooked medium and when it came out it was almost raw. Nicole took it back immediately and five minutes later my wife had a perfect lamb burger. On top of that Nicole gave us a free desert because of the burger and the wait.  My burger was great and with service like this who wouldn't come back!\"\n",
        "\n",
        "## rating = 3\n",
        "#rev = \"We have at least 2 birthday parties to attend here a year. The good part is the fact that the rides are contained in one secluded area so it's easy to keep and eye on the kiddos. The bad part is the cheesy arcade located within. Don't get me wrong I don't mind arcade games, what I detest is the stupid ticket program. You know, the one where the kids collect tickets from the various arcade games then turn them in for a prize? The program here is ridiculous. You have to get and ungodly amount of tickets to get anything, and that \\\"anything\\\" is usually something that can be purchased at the dollar store. The other annoying factor is that the ticket redemption booth is always understaffed and there you are...waiting in a line of indecisive kids so that your child can pick one of the many booby prizes available. It's torture I tell you! If they got rid of this ticket BS I would frequent this place 10 times more. My kids love the go carts and the kiddie roller coaster.\"\n",
        "\n",
        "## rating = 2\n",
        "rev = \"Will defiantly be reading more of aly's work\"\n",
        "revs = []\n",
        "word_atten_sen_number = 2\n",
        "\n",
        "text = BeautifulSoup(rev, \"lxml\")\n",
        "text = clean_str(text.get_text())\n",
        "revs.append(tokenize.sent_tokenize(text))\n",
        "\n",
        "for i, sentences in enumerate(revs):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j < MAX_SENTS:\n",
        "            wordTokens = text_to_word_sequence(sent)\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                    test_data[i,j,k] = tokenizer.word_index[word]\n",
        "                    k=k+1\n",
        "\n",
        "test_words = []\n",
        "test_data_words = np.zeros((1, MAX_SENT_LENGTH), dtype='int32')\n",
        "for i, sentences in enumerate(revs):\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if(j!=word_atten_sen_number):\n",
        "            continue\n",
        "        print(sent)\n",
        "        wordTokens = text_to_word_sequence(sent)\n",
        "        k=0\n",
        "        for _, word in enumerate(wordTokens):\n",
        "            if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
        "                test_data_words[i,k] = tokenizer.word_index[word]\n",
        "                k=k+1\n",
        "                test_words.append(word)\n",
        "        break"
      ],
      "metadata": {
        "id": "zNF0tmX7lLo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[2].output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4hoIIduz0J0",
        "outputId": "bd2594b4-4ff7-4daf-9f5d-41a6b438e6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 25, 100) dtype=float32 (created by layer 'bidirectional_3')>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_atten_output = K.function([model.layers[0].input], [model.layers[2].output])\n",
        "output = sent_atten_output([test_data])[0]  # test mode\n",
        "print(output[0].shape)"
      ],
      "metadata": {
        "id": "bkiyILM0lO4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a133e90-1daf-4e43-98b8-ae7e4e0a128f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-2].output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c6N5TV01v3I",
        "outputId": "2236ce21-db34-49cc-c9c7-a4d4ed734d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'att_layer_3')>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eij = np.tanh(np.multiply(model.layers[-2].get_weights(), np.transpose(output[0])))\n",
        "ai = np.exp(eij)\n",
        "for i in range(len(np.sum(test_data[0], axis = 1))):\n",
        "    if np.sum(test_data[0], axis = 1)[i] == 0:\n",
        "        ai[0][i] = 0.0\n",
        "weights = ai/np.sum(ai)\n",
        "\n",
        "print(weights[0][:len(revs[0])])\n",
        "print(len(revs[0]))"
      ],
      "metadata": {
        "id": "zuLmIPrIlQsU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6bcf5be0-0860-4b61-9033-496d0678dd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ae91fcbd92b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mai\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (100,25) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(revs[0], weights[0][:len(revs[0])]))"
      ],
      "metadata": {
        "id": "ILJ67yl8lSnT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "48799af2-4cc1-4c82-cd7a-d1508e5aec42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-f9affeabb451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrevs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrevs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_atten_output = K.function([sentEncoder.layers[0].input, K.learning_phase()], [sentEncoder.layers[2].output])\n",
        "out = word_atten_output([test_data_words, 0])[0]  # test mode\n",
        "print(out[0].shape)"
      ],
      "metadata": {
        "id": "lzNYJjRllUSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eij = np.tanh(np.dot(sentEncoder.layers[-1].get_weights(), np.transpose(out[0])))\n",
        "ai = np.exp(eij)\n",
        "count=0\n",
        "for i in range(len(np.sum(test_data_words, axis = 0))):\n",
        "    if np.sum(test_data_words, axis = 0)[i] == 0:\n",
        "        ai[0][i] = 0.0\n",
        "    else:\n",
        "        count+=1\n",
        "weights = ai/np.sum(ai)\n",
        "\n",
        "print(weights[0][:count])\n",
        "print(count)"
      ],
      "metadata": {
        "id": "rNmetaBYlWAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(test_words, weights[0][:count]))"
      ],
      "metadata": {
        "id": "Tj7rCWbslX2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}